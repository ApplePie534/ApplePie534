{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet streamlit torch transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0OAmS-VCUxa",
        "outputId": "f9cf3fa5-0c4a-4ce9-b2fe-93eb0df093b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-myfgSYTsSm",
        "outputId": "ce26ec56-f676-45ae-e69f-106846eb60d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Paths to your model and tokenizer in Google Drive\n",
        "    base_path = '/content/drive/MyDrive/Question Generator'\n",
        "    model_path = os.path.join(base_path, 't5_model_base')\n",
        "    tokenizer_path = os.path.join(base_path, 't5_tokenizer_base')\n",
        "\n",
        "    if not os.path.exists(model_path) or not os.path.exists(tokenizer_path):\n",
        "        raise FileNotFoundError(f\"Model or tokenizer not found in {base_path}. Please check the path.\")\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def get_question(context, answer, model, tokenizer, device):\n",
        "    text = f\"context: {context} answer: {answer}\"\n",
        "    encoding = tokenizer.encode_plus(text, max_length=512, padding='max_length',\n",
        "                                     truncation=True, return_tensors=\"pt\").to(device)\n",
        "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "    outs = model.generate(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          max_length=72,\n",
        "                          num_beams=5,\n",
        "                          early_stopping=True)\n",
        "    question = tokenizer.decode(outs[0], skip_special_tokens=True).replace(\"question:\", \"\").strip()\n",
        "    return question\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Question Generator\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "try:\n",
        "    model, tokenizer, device = load_model_and_tokenizer()\n",
        "    st.success(\"Model and tokenizer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading model and tokenizer: {str(e)}\")\n",
        "    st.stop()\n",
        "\n",
        "context = st.text_area(\"Enter the context:\", \"Donald Trump is an American media personality and businessman who served as the 45th president of the United States.\")\n",
        "answer = st.text_input(\"Enter the answer:\", \"Donald Trump\")\n",
        "\n",
        "if st.button(\"Generate Question\"):\n",
        "    with st.spinner(\"Generating question...\"):\n",
        "        question = get_question(context, answer, model, tokenizer, device)\n",
        "    st.success(\"Question generated!\")\n",
        "    st.write(\"Generated Question:\", question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8RbENbVCPWu",
        "outputId": "032ece74-e3f1-4b01-f1ff-cd6329cec2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuwBjygCCqkU",
        "outputId": "cc40b5f0-2246-4bef-9608-03b726d3aca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130.211.248.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3XgvKBVDWaF",
        "outputId": "f4726d2a-3e45-4cfe-ad3f-cea2902fb08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[?25l\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://130.211.248.45:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.312s\n",
            "your url is: https://busy-paws-shake.loca.lt\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    }
  ]
}