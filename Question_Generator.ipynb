{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApplePie534/ApplePie534/blob/main/Question_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VfKfPQvWVH4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f886f9-8908-4ff9-8c4a-b7fc978a3749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4891, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4790, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
            "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1672, in print\n",
            "    renderables = self._collect_renderables(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1551, in _collect_renderables\n",
            "    check_text()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1529, in check_text\n",
            "    append(sep_text.join(text))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 748, in join\n",
            "    def iter_text() -> Iterable[\"Text\"]:\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 309, in inner\n",
            "    return cached(*args, **kwds)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 1143, in __getitem__\n",
            "    params = tuple(_type_check(p, msg) for p in params)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 1143, in <genexpr>\n",
            "    params = tuple(_type_check(p, msg) for p in params)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 164, in _type_check\n",
            "    arg = _type_convert(arg, module=module, allow_special_forms=allow_special_forms)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 142, in _type_convert\n",
            "    return ForwardRef(arg, module=module, is_class=allow_special_forms)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet  datasets pyarrow tqdm transformers tokenizers sentencepiece pytorch-lightning torchtext streamlit nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile load_dataset.py\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "# Load the full datasets\n",
        "train_dataset = load_dataset('squad', split='train')\n",
        "full_valid_dataset = load_dataset('squad', split='validation')\n",
        "\n",
        "# Select only the first 1000 samples from the validation dataset\n",
        "valid_dataset = full_valid_dataset.select(range(1000))\n",
        "\n",
        "print(f\"Total Train Samples: {len(train_dataset)}\")\n",
        "print(f\"Total Validation Samples (full): {len(full_valid_dataset)}\")\n",
        "print(f\"Total Validation Samples (subset): {len(valid_dataset)}\")\n",
        "\n",
        "sample_validation_dataset = next(iter(valid_dataset))\n",
        "pprint (sample_validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvKDQBL35gLw",
        "outputId": "f679e4aa-c03b-4391-93ff-06a3878d07b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting load_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-fXe5zYVNDk",
        "outputId": "5624efa9-f9ef-47d1-d1c0-3608dd3d3a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_model.py\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pprint import pprint\n",
        "import copy\n",
        "import os\n",
        "from transformers import AdamW,T5ForConditionalGeneration,T5Tokenizer,get_linear_schedule_with_warmup\n",
        "import pytorch_lightning as pl\n",
        "from load_dataset import sample_validation_dataset,train_dataset,valid_dataset\n",
        "\n",
        "device  = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "pd.options.display.max_rows , pd.options.display.max_columns  = 100,100\n",
        "\n",
        "def create_pandas_dataset(data,\n",
        "                          answer_threshold=7,\n",
        "                          verbose = False):\n",
        "\n",
        "  ''' Create a Pandas Dataframe from hugging face dataset.\n",
        "  Params:\n",
        "        answer_threshold: Only consider those Question Answer pairs where the Answer is short.\n",
        "  '''\n",
        "  count_long ,count_short = 0 , 0\n",
        "  result_df  = pd.DataFrame(columns = ['context', 'answer','question'])\n",
        "  for index,val in enumerate(tqdm(data)):\n",
        "      passage = val['context']\n",
        "      question = val['question']\n",
        "      answer = val['answers']['text'][0]\n",
        "      no_of_words = len(answer.split())\n",
        "      if no_of_words >= answer_threshold:\n",
        "          count_long = count_long + 1\n",
        "          continue\n",
        "      else:\n",
        "          result_df.loc[count_short] = [passage] + [answer] + [question]\n",
        "          count_short = count_short + 1\n",
        "  if verbose:\n",
        "    return (result_df,\n",
        "            count_long,\n",
        "            count_short)\n",
        "  else:\n",
        "    return result_df\n",
        "\n",
        "context = sample_validation_dataset['context']\n",
        "question = sample_validation_dataset['question']\n",
        "answer = sample_validation_dataset['answers']['text'][0]\n",
        "print('---------------'*9)\n",
        "print('\\nBreaking it Down\\n')\n",
        "print (\"context:\",context)\n",
        "print (\"question:\",question)\n",
        "print (\"answer:\",answer)\n",
        "\n",
        "df_train , df_validation = create_pandas_dataset(train_dataset) , create_pandas_dataset(valid_dataset)\n",
        "print(f\"\\n Total Train Samples:{df_train.shape} , Total Validation Samples:{df_validation.shape}\")\n",
        "\n",
        "# Saving data for future use\n",
        "df_train.to_parquet('train_squad.parquet')\n",
        "df_validation.to_parquet('validation_squad.parquet')\n",
        "\n",
        "from transformers import AdamW,T5ForConditionalGeneration,T5Tokenizer,get_linear_schedule_with_warmup\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large',model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
        "\n",
        "class QuestionGenerationDataset(Dataset):\n",
        "    def __init__(self, tokenizer, filepath, max_len_inp=512,max_len_out=96):\n",
        "        self.path = filepath\n",
        "\n",
        "        self.passage_column = \"context\"\n",
        "        self.answer = \"answer\"\n",
        "        self.question = \"question\"\n",
        "\n",
        "        # self.data = pd.read_csv(self.path)\n",
        "        self.data = pd.read_parquet(self.path).iloc[:2000,:]\n",
        "\n",
        "        self.max_len_input = max_len_inp\n",
        "        self.max_len_output = max_len_out\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  #squeeze to get rid of the batch dimension\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # convert [batch,dim] to [dim]\n",
        "\n",
        "        labels = copy.deepcopy(target_ids)\n",
        "        labels [labels==0] = -100\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask,\"labels\":labels}\n",
        "\n",
        "    def _build(self):\n",
        "        for rownum,val in tqdm(self.data.iterrows()): # Iterating over the dataframe\n",
        "            passage,answer,target = val[self.passage_column],val[self.answer],val[self.question]\n",
        "\n",
        "            input_ = f\"context: {passage}  answer: {answer}\" # T5 Input format for question answering tasks\n",
        "            target = f\"question: {str(target)}\" # Output format we require\n",
        "\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len_input,padding='max_length',\n",
        "                truncation = True,return_tensors=\"pt\"\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len_output,padding='max_length',\n",
        "                truncation = True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)\n",
        "\n",
        "train_path = 'train_squad.parquet' # change this accordingly\n",
        "validation_path = 'validation_squad.parquet'\n",
        "train_dataset = QuestionGenerationDataset(t5_tokenizer,train_path)\n",
        "validation_dataset = QuestionGenerationDataset(t5_tokenizer,validation_path)\n",
        "\n",
        "# Data Sample\n",
        "\n",
        "train_sample = train_dataset[50] # thanks to __getitem__\n",
        "decoded_train_input = t5_tokenizer.decode(train_sample['source_ids'])\n",
        "decoded_train_output = t5_tokenizer.decode(train_sample['target_ids'])\n",
        "\n",
        "print(decoded_train_input)\n",
        "print(decoded_train_output)\n",
        "\n",
        "from torch.optim import AdamW\n",
        "import argparse\n",
        "\n",
        "class T5Tuner(pl.LightningModule):\n",
        "\n",
        "    def __init__(self,t5model, t5tokenizer,batchsize=4):\n",
        "        super().__init__()\n",
        "        self.model = t5model\n",
        "        self.tokenizer = t5tokenizer\n",
        "        self.batch_size = batchsize\n",
        "\n",
        "    def forward( self, input_ids, attention_mask=None,\n",
        "                decoder_attention_mask=None,\n",
        "                lm_labels=None):\n",
        "\n",
        "         outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "\n",
        "         return outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log('train_loss',loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log(\"val_loss\",loss)\n",
        "        return loss\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_dataset, batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(validation_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=3e-4, eps=1e-8)\n",
        "        return optimizer\n",
        "\n",
        "model = T5Tuner(t5_model,t5_tokenizer)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs = 3,accelerator=device)\n",
        "\n",
        "trainer.fit(model)\n",
        "\n",
        "# saving the model\n",
        "os.makedirs(\"t5_tokenizer\",exist_ok=True)\n",
        "os.makedirs(\"t5_trained_model\",exist_ok=True)\n",
        "model.model.save_pretrained('t5_trained_model')\n",
        "t5_tokenizer.save_pretrained('t5_tokenizer')\n",
        "\n",
        "trained_model_path = 't5_trained_model'\n",
        "trained_tokenizer = 't5_tokenizer'\n",
        "device = 'cpu'\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer)\n",
        "\n",
        "print(\"Training completed and model saved.\")\n",
        "\n",
        "context =\"President Donald Trump said and predicted that some states would reopen this month.\"\n",
        "answer = \"Donald Trump\"\n",
        "text = \"context: \"+context + \" \" + \"answer: \" + answer\n",
        "print(text)\n",
        "\n",
        "context =\"Since its topping out in 2013, One World Trade Center in New York City has been the tallest skyscraper in the United States.\"\n",
        "answer = \"World Trade Center\"\n",
        "text = \"context: \"+context + \" \" + \"answer: \" + answer\n",
        "print(text)\n",
        "\n",
        "encoding = tokenizer.encode_plus(text,max_length =512,padding='max_length',\n",
        "                                 truncation = True,\n",
        "                                 return_tensors=\"pt\").to(device)\n",
        "print (encoding.keys())\n",
        "input_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "model.eval()\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=72, # How long the generated questions should be\n",
        "    early_stopping=True,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=2\n",
        ")\n",
        "\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print(sent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx8cXCIrQVnh",
        "outputId": "3eab77ae-9bf1-4034-85ea-929d6cf41118"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Train Samples: 87599\n",
            "Total Validation Samples (full): 10570\n",
            "Total Validation Samples (subset): 1000\n",
            "{'answers': {'answer_start': [177, 177, 177],\n",
            "             'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']},\n",
            " 'context': 'Super Bowl 50 was an American football game to determine the '\n",
            "            'champion of the National Football League (NFL) for the 2015 '\n",
            "            'season. The American Football Conference (AFC) champion Denver '\n",
            "            'Broncos defeated the National Football Conference (NFC) champion '\n",
            "            'Carolina Panthers 24–10 to earn their third Super Bowl title. The '\n",
            "            \"game was played on February 7, 2016, at Levi's Stadium in the San \"\n",
            "            'Francisco Bay Area at Santa Clara, California. As this was the '\n",
            "            '50th Super Bowl, the league emphasized the \"golden anniversary\" '\n",
            "            'with various gold-themed initiatives, as well as temporarily '\n",
            "            'suspending the tradition of naming each Super Bowl game with '\n",
            "            'Roman numerals (under which the game would have been known as '\n",
            "            '\"Super Bowl L\"), so that the logo could prominently feature the '\n",
            "            'Arabic numerals 50.',\n",
            " 'id': '56be4db0acb8001400a502ec',\n",
            " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
            " 'title': 'Super_Bowl_50'}\n",
            "cuda\n",
            "---------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Breaking it Down\n",
            "\n",
            "context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "question: Which NFL team represented the AFC at Super Bowl 50?\n",
            "answer: Denver Broncos\n",
            "100% 87599/87599 [03:51<00:00, 378.11it/s]\n",
            "100% 1000/1000 [00:01<00:00, 987.56it/s]\n",
            "\n",
            " Total Train Samples:(78664, 3) , Total Validation Samples:(986, 3)\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 6.18MB/s]\n",
            "tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 7.33MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 1.21k/1.21k [00:00<00:00, 9.12MB/s]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "model.safetensors: 100% 2.95G/2.95G [00:19<00:00, 148MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 1.17MB/s]\n",
            "2000it [00:02, 676.25it/s]\n",
            "986it [00:01, 686.72it/s]\n",
            "context: In 1882, Albert Zahm (John Zahm's brother) built an early wind tunnel used to compare lift to drag of aeronautical models. Around 1899, Professor Jerome Green became the first American to send a wireless message. In 1931, Father Julius Nieuwland performed early work on basic reactions that was used to create neoprene. Study of nuclear physics at the university began with the building of a nuclear accelerator in 1936, and continues now partly through a partnership in the Joint Institute for Nuclear Astrophysics. answer: Professor Jerome Green</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "question: Which professor sent the first wireless message in the USA?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "2024-07-08 07:58:54.977137: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-08 07:58:54.977189: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-08 07:58:55.077014: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-08 07:58:55.159548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-08 07:58:56.256005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                       | Params | Mode\n",
            "------------------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 737 M  | eval\n",
            "------------------------------------------------------------\n",
            "737 M     Trainable params\n",
            "0         Non-trainable params\n",
            "737 M     Total params\n",
            "2,950.672 Total estimated model params size (MB)\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 0:   0% 0/500 [00:00<?, ?it/s] ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import streamlit as st\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from datasets import load_dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from load_dataset import sample_validation_dataset,train_dataset,valid_dataset\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5_trained_model').to(device)\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5_tokenizer')\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def get_question(context, answer, model, tokenizer, device):\n",
        "    text = f\"context: {context} answer: {answer}\"\n",
        "    encoding = tokenizer.encode_plus(text, max_length=512, padding='max_length',\n",
        "                                     truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "    outs = model.generate(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          max_length=72,\n",
        "                          num_beams=5,\n",
        "                          early_stopping=True)\n",
        "\n",
        "    question = tokenizer.decode(outs[0], skip_special_tokens=True).replace(\"question:\", \"\").strip()\n",
        "    return question\n",
        "\n",
        "def calculate_bleu_scores(generated_questions, reference_questions):\n",
        "    bleu_1 = []\n",
        "    bleu_2 = []\n",
        "    bleu_3 = []\n",
        "    bleu_4 = []\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    for gen, ref in zip(generated_questions, reference_questions):\n",
        "        reference = [ref.split()]\n",
        "        candidate = gen.split()\n",
        "\n",
        "        bleu_1.append(sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n",
        "        bleu_2.append(sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie))\n",
        "        bleu_3.append(sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie))\n",
        "        bleu_4.append(sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n",
        "\n",
        "    return {\n",
        "        'BLEU-1': sum(bleu_1) / len(bleu_1),\n",
        "        'BLEU-2': sum(bleu_2) / len(bleu_2),\n",
        "        'BLEU-3': sum(bleu_3) / len(bleu_3),\n",
        "        'BLEU-4': sum(bleu_4) / len(bleu_4)\n",
        "    }\n",
        "\n",
        "# Load your fine-tuned model and tokenizer\n",
        "model, tokenizer, device = load_model_and_tokenizer()\n",
        "\n",
        "# Generate questions and prepare references\n",
        "generated_questions = []\n",
        "reference_questions = []\n",
        "\n",
        "for sample in tqdm(valid_dataset):\n",
        "    context = sample['context']\n",
        "    answer = sample['answers']['text'][0]\n",
        "    reference_question = sample['question']\n",
        "\n",
        "    generated_question = get_question(context, answer, model, tokenizer, device)\n",
        "\n",
        "    generated_questions.append(generated_question)\n",
        "    reference_questions.append(reference_question)\n",
        "\n",
        "# Calculate BLEU scores\n",
        "bleu_scores = calculate_bleu_scores(generated_questions, reference_questions)\n",
        "\n",
        "print(\"BLEU Scores:\")\n",
        "for metric, score in bleu_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TEgGkGMnVav",
        "outputId": "ec6f1312-4276-4723-8553-fc2fd9664fcb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "100%|██████████| 1000/1000 [03:48<00:00,  4.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Scores:\n",
            "BLEU-1: 0.2854\n",
            "BLEU-2: 0.1900\n",
            "BLEU-3: 0.1354\n",
            "BLEU-4: 0.0986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Question Generator\")\n",
        "\n",
        "model, tokenizer, device = load_model_and_tokenizer()\n",
        "\n",
        "context = st.text_area(\"Enter the context:\", \"Donald Trump is an American media personality and businessman who served as the 45th president of the United States.\")\n",
        "answer = st.text_input(\"Enter the answer:\", \"Donald Trump\")\n",
        "\n",
        "if st.button(\"Generate Question\"):\n",
        "    with st.spinner(\"Generating question...\"):\n",
        "        question = get_question(context, answer, model, tokenizer, device)\n",
        "    st.success(\"Question generated!\")\n",
        "    st.write(\"Generated Question:\", question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2CMkJy5Le6R",
        "outputId": "fd9bc5ba-e19a-42f8-a9e0-69536b91e459"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5EAwVyCWRcq",
        "outputId": "13eaabf6-79eb-4afb-aaf6-1362303056a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.233.150.194\n"
          ]
        }
      ],
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6j1s65DWTCm",
        "outputId": "0ddcc3a8-eaab-4361-edd9-7bad4ce12eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.233.150.194:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.882s\n",
            "your url is: https://slow-oranges-watch.loca.lt\n"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_QVhdzoNax_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}